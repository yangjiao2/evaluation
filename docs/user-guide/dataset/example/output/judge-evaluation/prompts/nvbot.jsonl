{ "name": "single-ref-v1", "type": "single", "prompt_template": "Please act as an impartial judge and evaluate the quality of the responses provided by an AI assistants to the user question displayed below.\r\nYou will be given a reference answer and the assistant's answer.\r\n\r\nYour evaluation should consider the following dimensions: \r\n1. Correctness : Assess the accuracy of the assistant's answer by comparing it to the reference answer.\r\n2. Helpfulness : Evaluate the assistant's response based on its helpfulness. The criteria for helpfulness are as follows: {helpfulness}. Also evaluate the extent to which the response is actionable and can potentially enhance employee productivity.\r\nIf a citation is required in the response, use the citation provided in the assistant's answer for comparison with the Required Citations.\r\n3. Empathy: Evaluate whether the assistant's answer exhibits empathy, as per the defined empathy criteria: {empathy}\r\n4. Conciseness: Assess if the assistant's response is {short_ans}, without being overly verbose or omitting essential information.\r\n\r\nBegin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a 5-point Likert scale on all the dimensions.\r\nScoring guide:\r\n1 - Strongly Disagree (Very bad) - The answer is completely unrelated to the evaluation criteria defined. \r\n2 - Disagree (Bad) - The answer may only be partially meeting the evaluation criteria and is not of acceptable quality. \r\n3 - Neither disagree nor agree (neither bad nor good - okay) - The response is average. It meets some of the evaluation criterion, and misses others.\r\n4 - Agree (good) - The answer  satisfies the evaluation criteria but could be framed better for this task, or misses the detail required to make it perfect.\r\n5 - Strongly Agree (very good or excellent) - The answer not only satisfies, but exceeds the evaluation criteria.\r\n\r\nStriclty follow the JSON schema to output your response.\r\n\r\n{{\r\n  \"type\": \"object\",\r\n  \"properties\": {{\r\n    \"Explanation\": {{\r\n      \"type\": \"string\",\r\n      \"description\": \"A textual explanation or justification for the provided scores.\",\r\n    }},\r\n    \"Correctness Answer\": {{\r\n      \"type\": \"integer\",\r\n      \"minimum\": 1,\r\n      \"maximum\": 5\r\n    }},\r\n    \"Helpfulness\": {{\r\n      \"type\": \"integer\",\r\n      \"minimum\": 1,\r\n      \"maximum\": 5\r\n    }},\r\n    \"Empathy\": {{\r\n      \"type\": \"integer\",\r\n      \"minimum\": 1,\r\n      \"maximum\": 5\r\n    }},\r\n    \"Conciseness\": {{\r\n      \"type\": \"integer\",\r\n      \"minimum\": 1,\r\n      \"maximum\": 5\r\n    }}\r\n  }}\r\n}}\r\n\r\nAvoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.\r\n\r\nQuestion: {question}\r\n\r\nCorrect Answer: {correct_answer}\r\n\r\nRequired Citations: {required_citations}\r\n\r\nAssistant Answer: {answer}\r\n\r\nEvaluate:\r\n", "description": "for general LLM response evaluation", "category": "general", "output_format": "[[rating]]", "system_prompt": "You are a helpful assistant." }
