{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bot onboarding\n",
    "\n",
    "requirement:\n",
    "- bot definition: system, model\n",
    "- [suggested] evaluation dataset excel: question, ground-truth\n",
    "- llm-as-a-judge: judge prompt, judge parameters (for each question, if there are extra parameters, such as \"Require Citation\", prepare them in dataset excel)\n",
    "    \n",
    "Steps:\n",
    "\n",
    "\n",
    "- 1) create evaluation project (if not exist), optionally bot configuration can be added. If bot configuration is not provided, will use Model, System to fetch bot config.\n",
    "\n",
    "- 2) prepare dataset - queries, categories, reference answer\n",
    "\n",
    "- 3) generate answer based on NVplatform backend \n",
    "\n",
    "- 4) launch evaluation job\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a06f59603281974"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ! pip3 install ipywidgets\n",
    "# ! pip3 install huggingface"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e420eea2cf711d65",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fcfb0761acde7a49"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from service_library.constants import *\n",
    "import requests\n",
    "import huggingface_hub as hh\n",
    "import os\n",
    "import json\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "NVBOT_EVALUATION_URL: str = \"https://devbot-api.nvidia.com/evaluation\"\n",
    "HEADER = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b89e9bcc5eb8bc3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) create evaluation project\n",
    "\n",
    "require: system, model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8511cdb161457c4b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "756db5987c0947c0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ‚úçÔ∏è  Define each field \n",
    "project_name = \"\"  # example: \"scout_mixtral_agent\"\n",
    "\n",
    "project_description = \"\"  # example: \"scout mixtral agent\"\n",
    "status = \"\"  # example:\"healthy\"\n",
    "model = \"\"  # example:\"mixtral_agent\"\n",
    "system = \"\"  # example:\"scout\"\n",
    "\n",
    "email_subscription = \"\"  # example: \"\"\n",
    "nt_account = \"\" or NT_ACCOUNT_ID\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e884c1d52a25b5a0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Construct the dictionary using the parameters\n",
    "project_info = {\n",
    "    \"ProjectName\": project_name,\n",
    "    \"Description\": project_description,\n",
    "    \"Status\": status,\n",
    "    \"Model\": model,\n",
    "    \"System\": system,\n",
    "    \"EmailSubscription\": email_subscription,\n",
    "    \"NtAccount\": nt_account\n",
    "}\n",
    "\n",
    "# Print the dictionary\n",
    "print(project_info)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0c33c2872524ed6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"Verify evaluation project:\")\n",
    "print(project_info)\n",
    "\n",
    "print(\"\\nIf confirm on above info, execute next shell\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f4f3ceec8758d45",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Make the POST request\n",
    "response = requests.post(f\"{NVBOT_EVALUATION_URL}/evaluations_project\", headers=HEADER,\n",
    "                         data=json.dumps(project_info))\n",
    "project_id_created = None\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    print('evaluations_project post request successful.')\n",
    "    project_id_created = response.get(\"id\")\n",
    "    print(f\"Project created: {project_id_created}\")\n",
    "else:\n",
    "    print(f'failed with status code {response.status_code}.')\n",
    "    print(response.text)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3c58e6cd8c45dce",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) create dataset\n",
    "\n",
    "require:\n",
    "1. excel file with \"Query\", \"Answer\", optional columns include to provide as variables ingested into judge prompt. Example columns: \"Query\", \"Answer\", \"Required Citations\".\n",
    "2. judge module should be added to directory: prompt_module\": `eval_prompt_library/metrics_eval_prompt/`. For example, if python class module name: YourEvaluationPrompt. Inside this class, should have :\"output_format\", \"template\".\n",
    "output_format: which specifies the llm-as-a-judge evaluation output format\n",
    "template: Judge prompt to be used for evaluation, where variables in 1. can be inserted in snake case. For example: \"Required Citations\" will be inserted as \"{required_citations}\".\n",
    "\n",
    "judge prompts can also be provided in a json, and \"prompt_template\", \"output_format\".\n",
    "```\n",
    "    judge_template = {\n",
    "        \"name\": \"single-ref-v1\",\n",
    "        \"type\": \"single\",\n",
    "        \"prompt_template\": \"\",\n",
    "        \"description\": \"for general LLM response evaluation\",\n",
    "        \"category\": \"general\",\n",
    "        \"output_format\": \"\",\n",
    "        \"system_prompt\": \"You are a helpful assistant.\"\n",
    "    }\n",
    "```\n",
    "\n",
    "3. project_id\n",
    "4. dataset_name (optional)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "665a36bf39a3ac52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ‚úçÔ∏è Define each field \n",
    "question_column_name = \"\"  # example:\"Query\"\n",
    "reference_column_name = \"\"  # example:\"Answer\"\n",
    "additional_column_names_to_be_added_in_judge_prompt = []\n",
    "\n",
    "prompt_module_class_path_in_directory_eval_prompt_library = \"\"  # example: \"eval_prompt_library.metrics_eval_prompt.MetricsEvaluationPrompt\"\n",
    "# prompt_module also needs to have constants \"output_format, \"template\"\n",
    "\n",
    "judgement_prompt_in_prompt_module = \"eval_template_v2\"\n",
    "# additional columns are expected to be embedded in prompt as snake case  with curly braces {}\n",
    "# for example, Column value \"Require Citations\", should be converted to \"{require_citations}\n",
    "\n",
    "# LLM evaluation post-processing, example the following are shown in key-value pairs from judgement response, please verify in build.nvidia.com with provided judge prompts.\n",
    "parsing_scores = []\n",
    "# example: [\n",
    "#         \"Correctness Answer\",\n",
    "#         \"Helpfulness\",\n",
    "#         \"Empathy\",\n",
    "#         \"Conciseness\"\n",
    "#     ]\n",
    "parsing_text = []  # example: [\"Explanation\"]\n",
    "project_id = None  # example: 1\n",
    "if project_id_created is not None:\n",
    "    project_id = project_id_created\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c074d027cb044b00",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "column_map = {\n",
    "    \"question\": question_column_name,\n",
    "    \"reference\": reference_column_name,\n",
    "    \"additional_params\": additional_column_names_to_be_added_in_judge_prompt}\n",
    "\n",
    "judge_config = {\n",
    "    \"prompt_module\": prompt_module_class_path_in_directory_eval_prompt_library,\n",
    "    \"output_format\": \"output_format\",\n",
    "    \"template\": judgement_prompt_in_prompt_module,\n",
    "    \"scorers\": parsing_scores,\n",
    "    \"parse_keys\": parsing_text\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dad01e076d287b7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# prepare excel dataset\n",
    "\n",
    "print(\"Verify column_map is correct, columns exist in your excel dataset (case matters):\\n\")\n",
    "print(column_map)\n",
    "\n",
    "print(\n",
    "    \"Check judge_config is located in current path, such as `eval_prompt_library.metrics_eval_prompt` and constants `output_format` and `template` has been defined:\\n\")\n",
    "print(judge_config)\n",
    "\n",
    "print(\"Check project_id to be located in previous creation request, or define an existed project: \\n\")\n",
    "print(project_id)\n",
    "\n",
    "print(\"\\nIf confirm on above info (please double check with dataset provided (example screenshot below), execute POST request https://devbot-api.nvidia.com/evaluation/dataset.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "737ddaecc5299828"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![dataset_upload](./images/dataset_upload.png){: width=\"100px\"}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d105485d3d8dc1e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Copy response for dataset creation response\")\n",
    "# ‚úçÔ∏è Copy response after dataset created \n",
    "dataset_response = \"\"\n",
    "\n",
    "# example response:\n",
    "# \n",
    "# dataset_response = {\n",
    "#   \"Datastore\": {\n",
    "#     \"id\": \"dataset-Fqus9cyeLrMZGmUWgpCNxu\",\n",
    "#     \"path\": \"nvidia/nvhelp_groundtruth-llm_as_a_judge-0708-1547-Dlk\",\n",
    "#     \"name\": \"nvhelp_groundtruth-llm_as_a_judge-0708-1547-Dlk\",\n",
    "#     \"files\": [\n",
    "#       \"judge_prompt_parameters/correct_answer.json\",\n",
    "#       \"judge_prompt_parameters/empathy_expected.json\",\n",
    "#       \"judge_prompt_parameters/helpfulness_criteria.json\",\n",
    "#       \"judge_prompt_parameters/required_citations.json\",\n",
    "#       \"judge_prompt_parameters/short_answer_expected.json\",\n",
    "#       \"judge_prompts.jsonl\",\n",
    "#       \"question.jsonl\",\n",
    "#       \"reference_answer/reference.jsonl\"\n",
    "#     ]\n",
    "#   }\n",
    "# }\n",
    "\n",
    "datastore = dataset_response.get(\"Datastore\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a738b3cd3afa1896",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (optional) Download and verify dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c50ff6bc57d2f929"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import HTML\n",
    "\n",
    "dataset_id = dataset_response[\"Datastore\"].get(\"id\")\n",
    "url = f\"{NVBOT_EVALUATION_URL}/datasets/download/{dataset_id}\"\n",
    "headers = {\n",
    "    'accept': 'application/json'\n",
    "}\n",
    "\n",
    "download_response = requests.get(url, headers=headers)\n",
    "download_url = None\n",
    "if download_response.status_code == 200:\n",
    "    # print(f\"Dataset download request was successful: {download_response.url}\")\n",
    "    download_url = download_response.url\n",
    "    print(f\"Download dataset request completed: {download_url}\")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(f'<a href=\"{download_url}\" target=\"_blank\">Download Dataset</a>'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e68bb9231f33992c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# verify dataset structure\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd1287e56a0e50de",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) generate evaluation schema\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "def7a81daac90294"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ‚úçÔ∏è Define each field \n",
    "\n",
    "email_subscription_list = []\n",
    "env = \"\"  # example: dev, stg, prd\n",
    "model_name = \"\" # model_name, models available here: https://build.nvidia.com/search?term=Chat, example: mistralai/mixtral-8x22b-instruct-v0.1\"\n",
    "\n",
    "llm_as_a_judge_evaluator_payload = {\n",
    "    \"eval_type\":\n",
    "        \"llm_as_a_judge\",\n",
    "    \"eval_subtype\": \"mtbench\",\n",
    "    \"mode\": \"single\",\n",
    "    \"judge_inference_params\": {\n",
    "        \"top_p\": 0.1,\n",
    "        \"top_k\": 40,\n",
    "        \"temperature\": 0,\n",
    "        \"stop\": [],\n",
    "        \"tokens_to_generate\": 250\n",
    "    },\n",
    "    \"inference_params\": {\n",
    "        **EVAL_INFERENCE_DEFAULT,\n",
    "        \"extra_body\": {\n",
    "            \"project_id\": project_id,\n",
    "            \"env\": env\n",
    "        }\n",
    "    }\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b294d951431d5450",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"Verify below content for dataset config, which fills the template based on dataset creation response above\")\n",
    "\n",
    "dataset_config = {\n",
    "    \"Engine\": \"Datastore\",\n",
    "    \"Name\": dataset_response.get(\"name\"),\n",
    "    \"DatasetId\": dataset_response.get(\"id\"),\n",
    "    \"DatasetFolder\": dataset_response.get(\"name\"),\n",
    "    \"Files\": dataset_response.get(\"files\"),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b391513c6f094e2e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Verify the NemoEvaluator payload for LLM-as-a-judge\")\n",
    "\n",
    "nemo_evaluator = {\n",
    "    \"DatasetConfig\": dataset_config,\n",
    "    \"Evaluators\": [\n",
    "        {\n",
    "            \"name\": \"llm_evaluation\",\n",
    "            \"model\": {\n",
    "                \"llm_name\": model_name\n",
    "            },\n",
    "            \"evaluator_payload\": llm_as_a_judge_evaluator_payload,\n",
    "            \"judge_config\": judge_config\n",
    "        }\n",
    "    ]\n",
    "\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ddd6b03513dba4e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from data_models.api.run_maker import EvaluationSchema, Notification, NemoEvaluator\n",
    "\n",
    "evaluation_schema = EvaluationSchema(\n",
    "    Notification = Notification(\n",
    "        EmailRecipients = email_subscription_list\n",
    "    ),\n",
    "    NemoEvaluator = NemoEvaluator.model_validate(nemo_evaluator)        \n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea6631529f6d6594"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "project_name = \"eval_nvinfo_mixtral_agent_prd\"\n",
    "def create_json_in_codebase(data, file_name):\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "        \n",
    "create_json_in_codebase({\"EvaluationSchema\": evaluation_schema.dict()}, f\"../../asset/{project_name}.json\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36922fe925167e4f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print (f\"Great job üëè! Now please check on /asset/{project_name}.json, and commit the MR.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ceb5bebb2deaa92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
