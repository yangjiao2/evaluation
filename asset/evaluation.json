{
  "EvaluationSchema": {
    "Notification": {
      "EmailRecipients": [
      ]
    },
    "NemoEvaluator": {
      "DatasetConfig": {
        "Engine": "local",
        "RunFile": "tmp/eval_2.xlsx"
      },
      "Evaluators": [
        {
          "name": "custom_evaluation",
          "column_map": {
            "question": "Query",
            "reference": "Correct Answer",
            "answer": "Response"
          },
          "model": {
            "llm_name": "mixtral-8x22b"
          },
          "evaluator_payload": {
            "eval_type": "automatic",
            "eval_subtype": "custom_eval",
            "input_file": "",
            "inference_configs": [
              {
                "run_inference": false,
                "inference_params": {
                  "tokens_to_generate": 600,
                  "temperature": 0,
                  "top_k": 1
                }
              }
            ],
            "num_of_samples": -1,
            "scorers": [
              "accuracy",
              "bleu",
              "rouge",
              "em",
              "f1",
              "bert"
            ]
          }
        },
        {
          "name": "llm_evaluation",
          "column_map": {
            "question": "Query",
            "reference": "Correct Answer",
            "answer": "Response",
            "additional_params": [
              "Correct Answer",
              "Required Citations"
            ]
          },
          "model": {
            "llm_name": "mixtral-8x22b"
          },
          "evaluator_payload": {
            "eval_type": "llm_as_a_judge",
            "eval_subtype": "mtbench",
            "mode": "single",
            "judge_inference_params": {
              "top_p": 0.1,
              "top_k": 40,
              "temperature": 0,
              "stop": [],
              "tokens_to_generate": 600
            },
            "inference_params": {
              "tokens_to_generate": 600,
              "temperature": 0,
              "top_k": 1,
              "top_p": 0.75,
              "stop": [],
              "extra_body": {
                "project_id": 11,
                "env": "sandbox",
                "system": "nvinfo",
                "model": "mixtral_agent"
              }
            }
          },
          "judge_config": {
            "prompt_module": "eval_prompt_library.metrics_eval_prompt.MetricsEvaluationPrompt",
            "output_format": "output_format",
            "template": "eval_template_required_citations",
            "scorers": [
              "Correctness Answer",
              "Helpfulness"
            ],
            "parse_keys": [
              "Explanation"
            ]
          }
        }
      ]
    }
  }
}
