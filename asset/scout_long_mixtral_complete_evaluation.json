{
  "EvaluationSchema": {
    "Notification": {
      "EmailRecipients": [
      ]
    },
    "NemoEvaluator": {
      "DatasetConfig": {
        "Engine": ""
      },
      "Evaluators": [
        {
          "name": "custom_evaluation",
          "column_map": {
            "question": "Query",
            "reference": "Correct Answer",
            "answer": "Response",
            "category": "Category"
          },
          "model": {
            "llm_name": "mixtral-8x22b"
          },
          "evaluator_payload": {
            "eval_type": "automatic",
            "eval_subtype": "custom_eval",
            "input_file": "",
            "inference_configs": [
              {
                "run_inference": false,
                "inference_params": {
                  "tokens_to_generate": 600,
                  "temperature": 0,
                  "top_k": 1
                }
              }
            ],
            "num_of_samples": -1,
            "scorers": [
              "bleu",
              "rouge",
              "f1",
              "bert"
            ]
          }
        },
        {
          "name": "llm_evaluation",
          "column_map": {
            "question": "Query",
            "reference": "Correct Answer",
            "answer": "Response",
            "category": "Category",
            "additional_params": [
              "Correct Answer",
              "Required Citations"
            ]
          },
          "model": {
            "llm_name": "mistralai/mixtral-8x22b-instruct-v0.1"
          },
          "evaluator_payload": {
            "eval_type": "llm_as_a_judge",
            "eval_subtype": "mtbench",
            "mode": "single",
            "judge_inference_params": {
              "top_p": 0.1,
              "top_k": 40,
              "temperature": 0,
              "stop": [],
              "tokens_to_generate": 6000
            },
            "inference_params": {
              "tokens_to_generate": 6000,
              "temperature": 0,
              "top_k": 1,
              "top_p": 0.75,
              "stop": [],
              "extra_body": {
                "project_id": 17,
                "env": "stg",
                "system": "scout_long",
                "model": "mixtral"
              }
            }
          },
          "judge_config": {
            "prompt_module": "eval_prompt_library.metrics_eval_prompt.MetricsEvaluationPrompt",
            "output_format": "output_format",
            "template": "eval_template_required_citations",
            "scorers": [
              "Correctness Answer",
              "Helpfulness"
            ],
            "parse_keys": [
              "Explanation"
            ]
          }
        }
      ]
    }
  },
  "RegressionSchema": {
    "DatasetConfig": {
      "Engine": "s3",
      "DatasetFolder": "scout_mixtral",
      "Name": "scout_mixtral",
      "DatasetPath": "dataset/scout.xlsx"
    },
    "RunConfig": {
      "Inputs": [
        {
          "name": "",
          "type": "Attribute",
          "value": ""
        },
        {
          "name": "",
          "type": "Function",
          "value": "service_library.run.parser_library.input_parser.generate_orchestrator_fulfillment_request",
          "args": [
            "Query"
          ]
        }
      ],
      "Outputs": [
        {
          "name": "Response",
          "type": "Attribute",
          "value": "Response.Text"
        }
      ]
    }
  }
}