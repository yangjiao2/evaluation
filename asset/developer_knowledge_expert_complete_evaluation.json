{
  "EvaluationSchema": {
    "Notification": {
      "EmailRecipients": [
        "dix@nvidia.com",
        "yangj@nvidia.com"
      ]
    },
    "NemoEvaluator": {
      "Evaluators": [
        {
          "name": "custom_evaluation",
          "column_map": {
            "question": "Query",
            "reference": "Correct Answer",
            "answer": "Response"
          },
          "model": {
            "llm_name": "mixtral-8x22b"
          },
          "evaluator_payload": {
            "eval_type": "automatic",
            "eval_subtype": "custom_eval",
            "input_file": "",
            "inference_configs": [
              {
                "run_inference": false,
                "inference_params": {
                  "tokens_to_generate": 6000,
                  "temperature": 0,
                  "top_k": 1
                }
              }
            ],
            "num_of_samples": -1,
            "scorers": [
              "accuracy",
              "bleu",
              "rouge",
              "em",
              "f1",
              "bert"
            ]
          }
        },
        {
          "name": "llm_evaluation",
          "column_map": {
            "question": "Query",
            "reference": "Correct Answer",
            "answer": "Response",
            "additional_params": [
              "Correct Answer"
            ]
          },
          "model": {
            "llm_name": "mixtral-8x22b"
          },
          "evaluator_payload": {
            "eval_type": "llm_as_a_judge",
            "eval_subtype": "mtbench",
            "mode": "single",
            "judge_inference_params": {
              "top_p": 0.1,
              "top_k": 40,
              "temperature": 0,
              "stop": [],
              "tokens_to_generate": 600
            },
            "inference_params": {
              "top_p": 0.75,
              "top_k": 0,
              "temperature": 0,
              "stop": [],
              "tokens_to_generate": 25,
              "extra_body": {
              }
            }
          },
          "judge_config": {
            "prompt_module": "eval_prompt_library.metrics_eval_prompt.MetricsEvaluationPrompt",
            "output_format": "output_format",
            "template": "eval_template",
            "scorers": [
              "Correctness Answer"
            ],
            "parse_keys": [
              "Explanation"
            ]
          }
        }
      ]
    }
  },
  "RegressionSchema": {
    "DatasetConfig": {
      "Engine": "s3",
      "DatasetFolder": "developer_knowledge",
      "Name": "developer_knowledge",
      "DatasetPath": "dataset/developer_knowledge_expert_recurrent_benchmark_2025_04_06.xlsx"
    },
    "RunConfig": {
      "Inputs": [
        {
          "name": "",
          "type": "Attribute",
          "value": ""
        },
        {
          "name": "",
          "type": "Function",
          "value": "service_library.run.parser_library.input_parser.generate_fulfillment_request",
          "args": [
            "Query",
            "Category"
          ]
        }
      ],
      "Outputs": [
        {
          "name": "Response",
          "type": "Attribute",
          "value": "-1.developer_knowledge_expert.messages.-1.content"
        },
        {
          "type": "Function",
          "value": "service_library.run.parser_library.response_parser.response_citation_parser",
          "args": [
            "Response"
          ]
        },
        {
          "name": "Suggestion",
          "type": "Function",
          "value": "service_library.run.parser_library.response_parser.response_suggested_question_parser",
          "args": [
            "Response"
          ]
        },
        {
          "name": "Citation",
          "type": "Function",
          "value": "service_library.run.parser_library.response_parser.response_suggested_question_remover",
          "args": [
            "Citation"
          ]
        }
      ]
    }
  }
}